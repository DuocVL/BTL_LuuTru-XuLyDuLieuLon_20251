# Text Sentiment Analysis - Kubernetes Deployment

CÃ¡c file cáº¥u hÃ¬nh Kubernetes Ä‘á»ƒ deploy dá»± Ã¡n Text Sentiment Analysis lÃªn K8s cluster.

## ğŸ¯ Quick Start (Tá»« GitHub)

### Prerequisites
- Kubernetes cluster (Minikube, Docker Desktop, hoáº·c cloud provider)
- kubectl CLI Ä‘Ã£ cÃ i Ä‘áº·t
- Ãt nháº¥t 12GB RAM cho Minikube

### 1. Clone Repository
```bash
git clone https://github.com/mimiceolo/Text-Sentiment-Analysis.git
cd Text-Sentiment-Analysis
git checkout phase2
```

### 2. Start Minikube (náº¿u dÃ¹ng Minikube)
```bash
minikube start --memory=12288 --cpus=4 --disk-size=50g
```

### 3. Copy Sentiment API code vÃ o Minikube
```bash
# Táº¡o thÆ° má»¥c trong Minikube
minikube ssh "sudo mkdir -p /hosthome/api"

# Copy file Python
minikube cp api/sentiment_api.py /hosthome/api/sentiment_api.py
```

### 4. Deploy toÃ n bá»™ stack
```bash
# Apply táº¥t cáº£ configs
kubectl apply -f k8s/

# Náº¿u cÃ³ lá»—i namespace, cháº¡y láº¡i láº§n ná»¯a
kubectl apply -f k8s/
```

### 5. Kiá»ƒm tra deployment
```bash
# Xem tráº¡ng thÃ¡i pods
kubectl get pods -n sentiment-analysis

# Xem services
kubectl get svc -n sentiment-analysis
```

### 6. Truy cáº­p Sentiment API
```bash
# Minikube
minikube service sentiment-api -n sentiment-analysis

# Hoáº·c port-forward
kubectl port-forward -n sentiment-analysis svc/sentiment-api 5000:5000
```

Sau Ä‘Ã³ má»Ÿ browser: `http://localhost:5000`

---

## ğŸ“‹ Tá»•ng quan kiáº¿n trÃºc

Dá»± Ã¡n bao gá»“m cÃ¡c thÃ nh pháº§n sau:
- **MongoDB**: LÆ°u trá»¯ káº¿t quáº£ phÃ¢n tÃ­ch sentiment
- **Kafka**: Message queue cho streaming data (vá»›i Zookeeper)
- **HDFS**: Hadoop Distributed File System (NameNode + DataNode)
- **Spark**: Xá»­ lÃ½ real-time (Master + Workers)
- **Hadoop MapReduce**: Xá»­ lÃ½ batch
- **Flask API**: REST API Ä‘á»ƒ truy váº¥n káº¿t quáº£
- **Kafka Producer**: Stream tweets tá»« CSV

## ğŸš€ CÃ i Ä‘áº·t

### BÆ°á»›c 1: Táº¡o Namespace

```bash
kubectl apply -f k8s/namespace.yaml
```

### BÆ°á»›c 2: Deploy cÃ¡c thÃ nh pháº§n cÆ¡ sá»Ÿ háº¡ táº§ng

**MongoDB:**
```bash
kubectl apply -f k8s/mongodb-deployment.yaml
```

**Zookeeper & Kafka:**
```bash
kubectl apply -f k8s/zookeeper-deployment.yaml
kubectl apply -f k8s/kafka-deployment.yaml
```

**HDFS:**
```bash
kubectl apply -f k8s/hdfs-namenode-deployment.yaml
kubectl apply -f k8s/hdfs-datanode-deployment.yaml
```

**Spark:**
```bash
kubectl apply -f k8s/spark-master-deployment.yaml
kubectl apply -f k8s/spark-worker-deployment.yaml
```

### BÆ°á»›c 3: Deploy cÃ¡c á»©ng dá»¥ng

**Sentiment API:**
```bash
kubectl apply -f k8s/sentiment-api-deployment.yaml
```

**Kafka Producer:**
```bash
kubectl apply -f k8s/kafka-producer-deployment.yaml
```

### BÆ°á»›c 4: Deploy cÃ¡c Jobs

**Spark Jobs:**
```bash
kubectl apply -f k8s/spark-job.yaml
```

**Hadoop MapReduce Jobs:**
```bash
kubectl apply -f k8s/hadoop-job.yaml
```

### Hoáº·c deploy táº¥t cáº£ cÃ¹ng lÃºc:

```bash
kubectl apply -f k8s/
```

## ğŸ“Š Kiá»ƒm tra tráº¡ng thÃ¡i

```bash
# Xem táº¥t cáº£ pods
kubectl get pods -n sentiment-analysis

# Xem táº¥t cáº£ services
kubectl get services -n sentiment-analysis

# Xem logs cá»§a má»™t pod
kubectl logs -f <pod-name> -n sentiment-analysis

# Xem persistent volume claims
kubectl get pvc -n sentiment-analysis
```

## ğŸ”§ Cáº¥u hÃ¬nh

### Persistent Storage

Dá»± Ã¡n sá»­ dá»¥ng PersistentVolumeClaim cho:
- MongoDB: 20Gi
- Kafka: 10Gi
- Zookeeper: 5Gi
- HDFS NameNode: 20Gi
- HDFS DataNode: 50Gi

**LÆ°u Ã½**: Äáº£m báº£o cluster cá»§a báº¡n cÃ³ Ä‘á»§ storage vÃ  há»— trá»£ dynamic provisioning hoáº·c táº¡o PersistentVolume trÆ°á»›c.

### Resource Limits

Má»—i component Ä‘Ã£ Ä‘Æ°á»£c cáº¥u hÃ¬nh vá»›i resource requests vÃ  limits phÃ¹ há»£p. Äiá»u chá»‰nh trong file deployment náº¿u cáº§n:

```yaml
resources:
  requests:
    memory: "1Gi"
    cpu: "500m"
  limits:
    memory: "2Gi"
    cpu: "1000m"
```

### Scaling

**TÄƒng sá»‘ lÆ°á»£ng Spark Workers:**
```bash
kubectl scale deployment spark-worker --replicas=5 -n sentiment-analysis
```

**TÄƒng sá»‘ lÆ°á»£ng API instances:**
```bash
kubectl scale deployment sentiment-api --replicas=3 -n sentiment-analysis
```

## ğŸŒ Truy cáº­p Services

### Sentiment API

API Ä‘Æ°á»£c expose qua LoadBalancer. Láº¥y external IP:

```bash
kubectl get service sentiment-api -n sentiment-analysis
```

Sau Ä‘Ã³ truy cáº­p:
- Health check: `http://<EXTERNAL-IP>:5000/api/health`
- Dashboard: `http://<EXTERNAL-IP>:5000/dashboard`
- API docs: `http://<EXTERNAL-IP>:5000/`

### Spark Master UI

Port-forward Ä‘á»ƒ truy cáº­p Web UI:
```bash
kubectl port-forward svc/spark-master 8080:8080 -n sentiment-analysis
```
Truy cáº­p: `http://localhost:8080`

### HDFS NameNode UI

```bash
kubectl port-forward svc/hdfs-namenode 9870:9870 -n sentiment-analysis
```
Truy cáº­p: `http://localhost:9870`

### MongoDB

Káº¿t ná»‘i tá»« bÃªn trong cluster:
```
mongodb://mongodb.sentiment-analysis.svc.cluster.local:27017/sentiment_analysis
```

## ğŸ“¦ Upload dá»¯ liá»‡u vÃ  code

### Upload CSV data vÃ o Kafka Producer

```bash
# Copy CSV file vÃ o pod
kubectl cp data/training.csv sentiment-analysis/kafka-producer-<pod-id>:/data/

# Exec vÃ o pod vÃ  cháº¡y producer
kubectl exec -it kafka-producer-<pod-id> -n sentiment-analysis -- bash
python3 /app/tweet_producer.py --csv-file /data/training.csv
```

### Upload Spark application

```bash
# Build Spark application (trÃªn local)
cd Spark
sbt package

# Copy jar file vÃ o Spark master
kubectl cp target/scala-2.12/sentiment-analysis_2.12-1.0.jar \
  sentiment-analysis/spark-master-<pod-id>:/opt/bitnami/spark/jars/
```

### Upload Hadoop MapReduce JAR

```bash
# Compile Java code (trÃªn local)
cd Hadoop
javac -classpath $(hadoop classpath) NB.java
jar cf sentiment-nb.jar *.class

# Copy vÃ o HDFS namenode
kubectl cp sentiment-nb.jar sentiment-analysis/hdfs-namenode-<pod-id>:/tmp/
```

## ğŸ”„ Cháº¡y Jobs

### Cháº¡y Spark Job thá»§ cÃ´ng

```bash
kubectl create job --from=cronjob/spark-sentiment-job spark-manual-run -n sentiment-analysis
```

### Cháº¡y Hadoop Job thá»§ cÃ´ng

```bash
kubectl create job --from=cronjob/hadoop-mapreduce-job hadoop-manual-run -n sentiment-analysis
```

### Xem logs cá»§a Job

```bash
# Spark
kubectl logs -f job/spark-manual-run -n sentiment-analysis

# Hadoop
kubectl logs -f job/hadoop-manual-run -n sentiment-analysis
```

## ğŸ› Troubleshooting

### Pod khÃ´ng start

```bash
kubectl describe pod <pod-name> -n sentiment-analysis
kubectl logs <pod-name> -n sentiment-analysis
```

### ImagePullBackOff errors
Náº¿u pods bá»‹ lá»—i pull image:
```bash
# XÃ³a pod Ä‘á»ƒ retry
kubectl delete pod <pod-name> -n sentiment-analysis
```

### Memory issues (Pods Pending)
Náº¿u tháº¥y `Insufficient memory`:
```bash
# TÄƒng RAM cho Minikube
minikube stop
minikube delete
minikube start --memory=16384 --cpus=6
```

### Sentiment API khÃ´ng tÃ¬m tháº¥y file
```bash
# Kiá»ƒm tra file Ä‘Ã£ copy chÆ°a
minikube ssh "ls -la /hosthome/api/"

# Copy láº¡i náº¿u cáº§n
minikube cp api/sentiment_api.py /hosthome/api/sentiment_api.py
```

### Kafka CrashLoopBackOff
```bash
# Restart Kafka pod
kubectl delete pod -n sentiment-analysis -l app=kafka

# Äá»£i 30s Ä‘á»ƒ pod khá»Ÿi Ä‘á»™ng láº¡i
```

### Storage issues

```bash
# Xem PVC status
kubectl get pvc -n sentiment-analysis

# Describe PVC Ä‘á»ƒ xem lá»—i
kubectl describe pvc <pvc-name> -n sentiment-analysis
```

### XÃ³a toÃ n bá»™ deployment
```bash
kubectl delete namespace sentiment-analysis

# Hoáº·c dÃ¹ng script
bash k8s/undeploy.sh
```

## ğŸ“š TÃ i liá»‡u tham kháº£o

- [OVERVIEW.md](OVERVIEW.md) - Chi tiáº¿t kiáº¿n trÃºc há»‡ thá»‘ng
- [SETUP-CLUSTER.md](SETUP-CLUSTER.md) - HÆ°á»›ng dáº«n setup cluster chi tiáº¿t
- [Kubernetes Docs](https://kubernetes.io/docs/)
- [Apache Spark on K8s](https://spark.apache.org/docs/latest/running-on-kubernetes.html)

### Network issues

```bash
# Test káº¿t ná»‘i MongoDB tá»« API pod
kubectl exec -it sentiment-api-<pod-id> -n sentiment-analysis -- \
  ping mongodb

# Test Kafka connection
kubectl exec -it kafka-producer-<pod-id> -n sentiment-analysis -- \
  nc -zv kafka 9092
```

## ğŸ—‘ï¸ XÃ³a toÃ n bá»™ deployment

```bash
kubectl delete namespace sentiment-analysis
```

Hoáº·c xÃ³a tá»«ng thÃ nh pháº§n:
```bash
kubectl delete -f k8s/
```

## ğŸ“ LÆ°u Ã½ quan trá»ng

1. **Thá»© tá»± triá»ƒn khai**: Deploy infrastructure components trÆ°á»›c (MongoDB, Kafka, HDFS, Spark), sau Ä‘Ã³ má»›i deploy applications
2. **Init time**: Má»™t sá»‘ services cáº§n thá»i gian khá»Ÿi Ä‘á»™ng (Ä‘áº·c biá»‡t lÃ  HDFS vÃ  Spark). Äá»£i pods á»Ÿ tráº¡ng thÃ¡i Running trÆ°á»›c khi tiáº¿p tá»¥c
3. **Dependencies**: Äáº£m báº£o Zookeeper Ä‘Ã£ sáºµn sÃ ng trÆ°á»›c khi start Kafka
4. **ConfigMaps**: Cáº§n táº¡o ConfigMaps chá»©a code Python/Scala náº¿u muá»‘n tá»± Ä‘á»™ng mount code vÃ o pods
5. **Images**: CÃ³ thá»ƒ cáº§n build custom Docker images chá»©a sáºµn application code vÃ  dependencies

## ğŸ” Security Notes

- Cáº¥u hÃ¬nh hiá»‡n táº¡i khÃ´ng cÃ³ authentication/authorization (phÃ¹ há»£p cho development/testing)
- Cho production, cáº§n thÃªm:
  - MongoDB authentication
  - Kafka SASL/SSL
  - HDFS Kerberos
  - API authentication (JWT, OAuth)
  - Network Policies
  - Secrets cho sensitive data

## ğŸš€ Production Best Practices

1. Sá»­ dá»¥ng StatefulSets cho stateful apps (Kafka, HDFS)
2. Configure proper monitoring (Prometheus, Grafana)
3. Setup logging aggregation (ELK stack)
4. Use Ingress thay vÃ¬ LoadBalancer cho API
5. Implement auto-scaling (HPA)
6. Regular backups cho MongoDB vÃ  HDFS
7. Use Helm charts Ä‘á»ƒ quáº£n lÃ½ deployments

## ğŸ“š TÃ i liá»‡u tham kháº£o

- [Kubernetes Documentation](https://kubernetes.io/docs/)
- [Spark on Kubernetes](https://spark.apache.org/docs/latest/running-on-kubernetes.html)
- [Hadoop on Kubernetes](https://hadoop.apache.org/docs/stable/hadoop-yarn/hadoop-yarn-site/DockerContainers.html)
